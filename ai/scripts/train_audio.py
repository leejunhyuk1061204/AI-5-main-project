# ai/scripts/train_audio.py
"""
AST ê¸°ê³„ ì†ŒìŒ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ë„êµ¬ (Audio Trainer)

[ì—­í• ]
1. ì†Œë¦¬ ê¸°ë°˜ ì§„ë‹¨: ì°¨ëŸ‰ì—ì„œ ë°œìƒí•˜ëŠ” ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê¸°ê³„ì  ê³ ì¥(ë…¸í‚¹, ì‹¤í™” ë“±)ì„ ë¶„ë¥˜í•˜ëŠ” AST ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
2. ì „ì²˜ë¦¬ ìë™í™”: ì˜¤ë””ì˜¤ íŒŒì¼ì„ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ íŠ¹ì§•(Feature)ìœ¼ë¡œ ìë™ ë³€í™˜í•˜ë©°, Windows í™˜ê²½ì—ì„œì˜ librosa ë¡œë”© ì´ìŠˆë¥¼ í•´ê²°í–ˆìŠµë‹ˆë‹¤.
3. ì„±ëŠ¥ ë¦¬í¬íŠ¸: í•™ìŠµ ì „(Baseline)ê³¼ í•™ìŠµ í›„(Final)ì˜ ì •í™•ë„ë¥¼ ë¹„êµí•˜ì—¬ ëª¨ë¸ì˜ ê°œì„  ì •ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.

[ì‚¬ìš©ë²•]
python ai/scripts/train_audio.py --mode all --epochs 10
"""
import argparse
import os
import torch
import numpy as np
import boto3
import evaluate
from transformers import ASTForAudioClassification, ASTFeatureExtractor, Trainer, TrainingArguments
from datasets import Dataset, Audio
from sklearn.model_selection import train_test_split

# =============================================================================
# [ì„¤ì •] ê²½ë¡œ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
# =============================================================================
MODEL_NAME = "MIT/ast-finetuned-audioset-10-10-0.4593"
OUTPUT_DIR = "./ai/runs/audio_model"
SAVE_PATH = "./ai/weights/audio/best_ast_model"

LABEL_MAP = {
    "benz_normal": "Normal",
    "audi_normal": "Normal",
    "ì •ìƒ": "Normal",
    "Knocking": "Engine_Knocking",
    "Misfire": "Engine_Misfire",
    "Belt": "Belt_Issue",
    "ì†ŒìŒ": "Abnormal_Noise"
}

# =============================================================================
# ì „ì—­ ë³€ìˆ˜ (ë°ì´í„° ì¤€ë¹„ í›„ ê³µìœ )
# =============================================================================
train_dataset = None
eval_dataset = None
test_dataset = None
label2id = None
id2label = None
labels = None
feature_extractor = None

# =============================================================================
# 1. ë°ì´í„° ì¤€ë¹„
# =============================================================================
def prepare_data():
    global train_dataset, eval_dataset, test_dataset, label2id, id2label, labels, feature_extractor
    
    print("\n" + "="*50)
    print("[Step 1] ë°ì´í„° ì¤€ë¹„ ì‹œì‘...")
    print("="*50)
    
    # =============================================================================
    # ë¡œì»¬ ë°ì´í„° í´ë”ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
    # í´ë” êµ¬ì¡°:
    #   ai/data/ast/
    #     â”œâ”€â”€ normal/          (ì •ìƒ ì—”ì§„ìŒ: .wav íŒŒì¼ë“¤)
    #     â”œâ”€â”€ knocking/        (ë…¸í‚¹ ì†Œë¦¬)
    #     â”œâ”€â”€ belt/            (ë²¨íŠ¸ ì†Œë¦¬)
    #     â”œâ”€â”€ misfire/         (ì‹¤í™” ì†Œë¦¬)
    #     â””â”€â”€ ... (ì¶”ê°€ ë¼ë²¨ í´ë”)
    # =============================================================================
    LOCAL_DATA_DIR = "./ai/data/ast"
    
    if not os.path.exists(LOCAL_DATA_DIR):
        os.makedirs(LOCAL_DATA_DIR, exist_ok=True)
        print(f"[Warning] ë°ì´í„° í´ë”ê°€ ì—†ì–´ì„œ ìƒì„±í–ˆìŠµë‹ˆë‹¤: {LOCAL_DATA_DIR}")
        print(f"         ì—¬ê¸°ì— ë¼ë²¨ë³„ í•˜ìœ„ í´ë”(normal, knocking ë“±)ë¥¼ ë§Œë“¤ê³  ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        print(f"         ì§€ì› í˜•ì‹: .wav, .mp3, .m4a, .ogg, .flac")
        return False
    
    DATA_SOURCE_PATHS = [LOCAL_DATA_DIR]
    
    # (ì„ íƒì ) S3 ìˆ˜ì§‘ ë°ì´í„°ë„ ì¶”ê°€ë¡œ ë¶ˆëŸ¬ì˜¤ê¸° - ë‚˜ì¤‘ì— Active Learning ë•Œ ì‚¬ìš©
    try:
        s3_download_dir = "./ai/data/s3_audio"
        s3 = boto3.client('s3')
        bucket_name = os.getenv("S3_BUCKET_NAME", "car-sentry-data")
        
        objects = s3.list_objects_v2(Bucket=bucket_name, Prefix="dataset/audio/")
        if 'Contents' in objects:
            count = 0
            for obj in objects['Contents']:
                key = obj['Key']
                # ë‹¤ì–‘í•œ ì˜¤ë””ì˜¤ í˜•ì‹ ì§€ì›
                audio_extensions = ('.wav', '.mp3', '.m4a', '.ogg', '.flac')
                if not key.lower().endswith(audio_extensions): continue
                
                rel_path = key.replace("dataset/audio/", "")
                local_path = os.path.join(s3_download_dir, rel_path)
                
                if not os.path.exists(local_path):
                    os.makedirs(os.path.dirname(local_path), exist_ok=True)
                    s3.download_file(bucket_name, key, local_path)
                    count += 1
            
            if count > 0:
                print(f"[Info] S3ì—ì„œ {count}ê°œì˜ ì‹ ê·œ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            DATA_SOURCE_PATHS.append(s3_download_dir)
            
    except Exception as e:
        print(f"[Info] S3 ì—°ê²° ê±´ë„ˆëœ€ (ë¡œì»¬ ë°ì´í„°ë§Œ ì‚¬ìš©): {e}")

    # ë°ì´í„° ìˆ˜ì§‘
    data_list = []
    for base_path in DATA_SOURCE_PATHS:
        if not os.path.exists(base_path):
            continue
        for root, dirs, files in os.walk(base_path):
            for file in files:
                # ë‹¤ì–‘í•œ ì˜¤ë””ì˜¤ í˜•ì‹ ì§€ì›
                audio_extensions = ('.wav', '.mp3', '.m4a', '.ogg', '.flac')
                if file.lower().endswith(audio_extensions):
                    folder_name = os.path.basename(root)
                    label = LABEL_MAP.get(folder_name, folder_name)
                    full_path = os.path.join(root, file)
                    data_list.append({"audio": full_path, "label": label})
    
    print(f"[Info] ì´ {len(data_list)}ê°œì˜ ì˜¤ë””ì˜¤ íŒŒì¼ ë°œê²¬")
    
    if len(data_list) == 0:
        print("[Error] ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    # ë¼ë²¨ ì¸ì½”ë”©
    labels = list(set([x['label'] for x in data_list]))
    label2id = {label: i for i, label in enumerate(labels)}
    id2label = {i: label for i, label in enumerate(labels)}
    print(f"[Info] ê°ì§€ëœ ë¼ë²¨({len(labels)}ê°œ): {labels}")
    
    # 7:2:1 ë¶„í• 
    train_val, test_data = train_test_split(
        data_list, test_size=0.2, stratify=[x['label'] for x in data_list], random_state=42
    )
    train_data, val_data = train_test_split(
        train_val, test_size=0.125, stratify=[x['label'] for x in train_val], random_state=42
    )
    
    print(f"[Info] ë°ì´í„° ë¶„í• : Train={len(train_data)}, Valid={len(val_data)}, Test={len(test_data)}")
    
    # Feature Extractor ë¡œë“œ
    feature_extractor = ASTFeatureExtractor.from_pretrained(MODEL_NAME)
    
    # librosaë¡œ ì§ì ‘ ì˜¤ë””ì˜¤ ë¡œë”© (torchcodec ìš°íšŒ!)
    import librosa
    
    def load_audio_with_librosa(file_path, target_sr=16000):
        """librosaë¡œ ì˜¤ë””ì˜¤ ë¡œë“œ (Windows í˜¸í™˜)"""
        try:
            audio_array, _ = librosa.load(file_path, sr=target_sr)
            return audio_array
        except Exception as e:
            print(f"[Warning] ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨: {file_path} - {e}")
            return None
    
    def preprocess_batch(data_list, desc="Processing"):
        """ë°°ì¹˜ ì „ì²˜ë¦¬ (librosa ì‚¬ìš©)"""
        processed_data = []
        
        for item in data_list:
            audio_array = load_audio_with_librosa(item["audio"])
            if audio_array is None:
                continue
            
            # Feature extraction
            inputs = feature_extractor(
                audio_array, 
                sampling_rate=16000, 
                return_tensors="pt", 
                padding="max_length"
            )
            
            processed_data.append({
                "input_values": inputs["input_values"].squeeze(0).numpy(),
                "labels": label2id[item["label"]]
            })
        
        return processed_data
    
    # Dataset ìƒì„± ë° ì „ì²˜ë¦¬
    print("[Info] ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ (librosa ì‚¬ìš©)...")
    
    train_processed = preprocess_batch(train_data, "Train")
    val_processed = preprocess_batch(val_data, "Valid")
    test_processed = preprocess_batch(test_data, "Test")
    
    print(f"[Info] ì „ì²˜ë¦¬ ì™„ë£Œ: Train={len(train_processed)}, Valid={len(val_processed)}, Test={len(test_processed)}")
    
    # HuggingFace Datasetìœ¼ë¡œ ë³€í™˜
    train_dataset = Dataset.from_list(train_processed)
    eval_dataset = Dataset.from_list(val_processed)
    test_dataset = Dataset.from_list(test_processed)
    
    # Tensor í˜•ì‹ ì„¤ì •
    train_dataset.set_format(type="torch", columns=["input_values", "labels"])
    eval_dataset.set_format(type="torch", columns=["input_values", "labels"])
    test_dataset.set_format(type="torch", columns=["input_values", "labels"])
    
    print("[âœ“] ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
    return True

# =============================================================================
# 2. ì´ˆê¸° ëª¨ë¸ ì •ë°€ë„ ì¸¡ì • (Baseline)
# =============================================================================
def evaluate_baseline():
    print("\n" + "="*50)
    print("[Step 2] ì´ˆê¸° ëª¨ë¸(Baseline) ì •ë°€ë„ ì¸¡ì •...")
    print("="*50)
    
    if test_dataset is None:
        print("[Error] ë¨¼ì € ë°ì´í„°ë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš” (--mode all ë˜ëŠ” prepare_data í˜¸ì¶œ)")
        return
    
    # ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ (Fine-tuning ì „)
    model = ASTForAudioClassification.from_pretrained(
        MODEL_NAME,
        num_labels=len(labels),
        label2id=label2id,
        id2label=id2label,
        ignore_mismatched_sizes=True
    )
    
    accuracy_metric = evaluate.load("accuracy")
    
    def compute_metrics(eval_pred):
        predictions, labels_arr = eval_pred
        predictions = np.argmax(predictions, axis=1)
        return accuracy_metric.compute(predictions=predictions, references=labels_arr)
    
    training_args = TrainingArguments(
        output_dir="./ai/runs/baseline_eval",
        per_device_eval_batch_size=8,
        push_to_hub=False,
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        eval_dataset=test_dataset,
        compute_metrics=compute_metrics,
    )
    
    metrics = trainer.evaluate()
    
    print("\n" + "="*40)
    print(f"ğŸ¯ ì´ˆê¸° ëª¨ë¸ ì •í™•ë„(Baseline): {metrics['eval_accuracy']:.4f}")
    print("="*40 + "\n")
    
    return metrics['eval_accuracy']

# =============================================================================
# 3. ëª¨ë¸ í•™ìŠµ
# =============================================================================
def train_model(epochs=10):
    print("\n" + "="*50)
    print(f"[Step 3] ëª¨ë¸ í•™ìŠµ ì‹œì‘ ({epochs} epochs)...")
    print("="*50)
    
    if train_dataset is None:
        print("[Error] ë¨¼ì € ë°ì´í„°ë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš”")
        return None
    
    model = ASTForAudioClassification.from_pretrained(
        MODEL_NAME,
        num_labels=len(labels),
        label2id=label2id,
        id2label=id2label,
        ignore_mismatched_sizes=True
    )
    
    accuracy_metric = evaluate.load("accuracy")
    
    def compute_metrics(eval_pred):
        predictions, labels_arr = eval_pred
        predictions = np.argmax(predictions, axis=1)
        return accuracy_metric.compute(predictions=predictions, references=labels_arr)
    
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=8,
        num_train_epochs=epochs,
        learning_rate=3e-5,
        logging_dir='./logs',
        eval_strategy="epoch",  # ìµœì‹  ë²„ì „ í˜¸í™˜
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",
        push_to_hub=False,
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        compute_metrics=compute_metrics,
    )
    
    print("í•™ìŠµ ì‹œì‘...")
    trainer.train()
    
    # ëª¨ë¸ ì €ì¥
    os.makedirs(SAVE_PATH, exist_ok=True)
    model.save_pretrained(SAVE_PATH)
    feature_extractor.save_pretrained(SAVE_PATH)
    print(f"[âœ“] ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {SAVE_PATH}")
    
    return trainer

# =============================================================================
# 4. ìµœì¢… ëª¨ë¸ í…ŒìŠ¤íŠ¸
# =============================================================================
def evaluate_final():
    print("\n" + "="*50)
    print("[Step 4] ìµœì¢… ëª¨ë¸ ì •ë°€ë„ ì¸¡ì •...")
    print("="*50)
    
    if not os.path.exists(SAVE_PATH):
        print(f"[Error] í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤: {SAVE_PATH}")
        print("ë¨¼ì € --mode train ìœ¼ë¡œ í•™ìŠµì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    if test_dataset is None:
        print("[Error] ë¨¼ì € ë°ì´í„°ë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš”")
        return
    
    # í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
    model = ASTForAudioClassification.from_pretrained(SAVE_PATH)
    
    accuracy_metric = evaluate.load("accuracy")
    
    def compute_metrics(eval_pred):
        predictions, labels_arr = eval_pred
        predictions = np.argmax(predictions, axis=1)
        return accuracy_metric.compute(predictions=predictions, references=labels_arr)
    
    training_args = TrainingArguments(
        output_dir="./ai/runs/final_eval",
        per_device_eval_batch_size=8,
        push_to_hub=False,
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        eval_dataset=test_dataset,
        compute_metrics=compute_metrics,
    )
    
    metrics = trainer.evaluate()
    
    print("\n" + "="*40)
    print(f"ğŸ¯ ìµœì¢… ëª¨ë¸ ì •í™•ë„(Final): {metrics['eval_accuracy']:.4f}")
    print("="*40 + "\n")
    
    return metrics['eval_accuracy']

# =============================================================================
# Main
# =============================================================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="AST Audio Model Training Script")
    parser.add_argument("--mode", type=str, default="all",
                        choices=["baseline", "train", "test", "all"],
                        help="ì‹¤í–‰ ëª¨ë“œ: baseline(ì´ˆê¸°), train(í•™ìŠµ), test(í…ŒìŠ¤íŠ¸), all(ì „ì²´)")
    parser.add_argument("--epochs", type=int, default=10,
                        help="í•™ìŠµ ì—í­ ìˆ˜ (ê¸°ë³¸ê°’: 10)")
    
    args = parser.parse_args()
    
    print(f"\nğŸš€ Audio Training Script ì‹œì‘ (mode={args.mode}, epochs={args.epochs})")
    
    # ë°ì´í„° ì¤€ë¹„ (ëª¨ë“  ëª¨ë“œì—ì„œ í•„ìš”)
    if not prepare_data():
        exit(1)
    
    if args.mode == "baseline":
        evaluate_baseline()
    
    elif args.mode == "train":
        train_model(epochs=args.epochs)
    
    elif args.mode == "test":
        evaluate_final()
    
    elif args.mode == "all":
        baseline_acc = evaluate_baseline()
        train_model(epochs=args.epochs)
        final_acc = evaluate_final()
        
        print("\n" + "="*50)
        print("ğŸ“Š ì •í™•ë„ ë¹„êµ")
        print("="*50)
        print(f"   ì´ˆê¸° ëª¨ë¸(Baseline): {baseline_acc:.4f}")
        print(f"   ìµœì¢… ëª¨ë¸(Final):    {final_acc:.4f}")
        print(f"   í–¥ìƒë„:              +{(final_acc - baseline_acc)*100:.2f}%")
        print("="*50 + "\n")
    
    print("âœ… ì™„ë£Œ!")