#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Audio Training Script (AST Model)
Usage:
    python train_audio.py --mode baseline  # ì´ˆê¸° ëª¨ë¸ ì •ë°€ë„ë§Œ ì¸¡ì •
    python train_audio.py --mode train     # í•™ìŠµë§Œ ì‹¤í–‰
    python train_audio.py --mode test      # ìµœì¢… ëª¨ë¸ í…ŒìŠ¤íŠ¸ë§Œ
    python train_audio.py --mode all       # ì „ì²´ ì‹¤í–‰ (ê¸°ë³¸ê°’)
"""
import argparse
import kagglehub
import os
import torch
import numpy as np
from transformers import ASTForAudioClassification, ASTFeatureExtractor, Trainer, TrainingArguments
from datasets import Dataset, Audio
from sklearn.model_selection import train_test_split
import evaluate

# =============================================================================
# [ì„¤ì •] ê²½ë¡œ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
# =============================================================================
MODEL_NAME = "MIT/ast-finetuned-audioset-10-10-0.4593"
OUTPUT_DIR = "./ai/runs/audio_model"
SAVE_PATH = "./ai/weights/audio/best_ast_model"

LABEL_MAP = {
    "benz_normal": "Normal",
    "audi_normal": "Normal",
    "ì •ìƒ": "Normal",
    "Knocking": "Engine_Knocking",
    "Misfire": "Engine_Misfire",
    "Belt": "Belt_Issue",
    "ì†ŒìŒ": "Abnormal_Noise"
}

# =============================================================================
# ì „ì—­ ë³€ìˆ˜ (ë°ì´í„° ì¤€ë¹„ í›„ ê³µìœ )
# =============================================================================
train_dataset = None
eval_dataset = None
test_dataset = None
label2id = None
id2label = None
labels = None
feature_extractor = None

# =============================================================================
# 1. ë°ì´í„° ì¤€ë¹„
# =============================================================================
def prepare_data():
    global train_dataset, eval_dataset, test_dataset, label2id, id2label, labels, feature_extractor
    
    print("\n" + "="*50)
    print("[Step 1] ë°ì´í„° ì¤€ë¹„ ì‹œì‘...")
    print("="*50)
    
    DATA_SOURCE_PATHS = []
    
    # Kaggle ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
    try:
        kaggle_path = kagglehub.dataset_download("janboubiabderrahim/vehicle-sounds-dataset")
        print(f"[Info] Kaggle ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {kaggle_path}")
        DATA_SOURCE_PATHS.append(kaggle_path)
    except Exception as e:
        print(f"[Warning] Kaggle ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ë°ì´í„° ìˆ˜ì§‘
    data_list = []
    for base_path in DATA_SOURCE_PATHS:
        if not os.path.exists(base_path):
            continue
        for root, dirs, files in os.walk(base_path):
            for file in files:
                if file.lower().endswith('.wav'):
                    folder_name = os.path.basename(root)
                    label = LABEL_MAP.get(folder_name, folder_name)
                    full_path = os.path.join(root, file)
                    data_list.append({"audio": full_path, "label": label})
    
    print(f"[Info] ì´ {len(data_list)}ê°œì˜ ì˜¤ë””ì˜¤ íŒŒì¼ ë°œê²¬")
    
    if len(data_list) == 0:
        print("[Error] ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    # ë¼ë²¨ ì¸ì½”ë”©
    labels = list(set([x['label'] for x in data_list]))
    label2id = {label: i for i, label in enumerate(labels)}
    id2label = {i: label for i, label in enumerate(labels)}
    print(f"[Info] ê°ì§€ëœ ë¼ë²¨({len(labels)}ê°œ): {labels}")
    
    # 7:2:1 ë¶„í• 
    train_val, test_data = train_test_split(
        data_list, test_size=0.2, stratify=[x['label'] for x in data_list], random_state=42
    )
    train_data, val_data = train_test_split(
        train_val, test_size=0.125, stratify=[x['label'] for x in train_val], random_state=42
    )
    
    print(f"[Info] ë°ì´í„° ë¶„í• : Train={len(train_data)}, Valid={len(val_data)}, Test={len(test_data)}")
    
    # Feature Extractor ë¡œë“œ
    feature_extractor = ASTFeatureExtractor.from_pretrained(MODEL_NAME)
    
    def preprocess_function(examples):
        audio_arrays = [x["array"] for x in examples["audio"]]
        inputs = feature_extractor(audio_arrays, sampling_rate=16000, return_tensors="pt", padding="max_length")
        return inputs
    
    # Dataset ìƒì„± ë° ì „ì²˜ë¦¬
    print("[Info] ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    train_ds = Dataset.from_list(train_data).cast_column("audio", Audio(sampling_rate=16000))
    val_ds = Dataset.from_list(val_data).cast_column("audio", Audio(sampling_rate=16000))
    test_ds = Dataset.from_list(test_data).cast_column("audio", Audio(sampling_rate=16000))
    
    train_dataset = train_ds.map(preprocess_function, batched=True)
    eval_dataset = val_ds.map(preprocess_function, batched=True)
    test_dataset = test_ds.map(preprocess_function, batched=True)
    
    print("[âœ“] ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
    return True

# =============================================================================
# 2. ì´ˆê¸° ëª¨ë¸ ì •ë°€ë„ ì¸¡ì • (Baseline)
# =============================================================================
def evaluate_baseline():
    print("\n" + "="*50)
    print("[Step 2] ì´ˆê¸° ëª¨ë¸(Baseline) ì •ë°€ë„ ì¸¡ì •...")
    print("="*50)
    
    if test_dataset is None:
        print("[Error] ë¨¼ì € ë°ì´í„°ë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš” (--mode all ë˜ëŠ” prepare_data í˜¸ì¶œ)")
        return
    
    # ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ (Fine-tuning ì „)
    model = ASTForAudioClassification.from_pretrained(
        MODEL_NAME,
        num_labels=len(labels),
        label2id=label2id,
        id2label=id2label,
        ignore_mismatched_sizes=True
    )
    
    accuracy_metric = evaluate.load("accuracy")
    
    def compute_metrics(eval_pred):
        predictions, labels_arr = eval_pred
        predictions = np.argmax(predictions, axis=1)
        return accuracy_metric.compute(predictions=predictions, references=labels_arr)
    
    training_args = TrainingArguments(
        output_dir="./ai/runs/baseline_eval",
        per_device_eval_batch_size=8,
        push_to_hub=False,
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        eval_dataset=test_dataset,
        compute_metrics=compute_metrics,
    )
    
    metrics = trainer.evaluate()
    
    print("\n" + "="*40)
    print(f"ğŸ¯ ì´ˆê¸° ëª¨ë¸ ì •í™•ë„(Baseline): {metrics['eval_accuracy']:.4f}")
    print("="*40 + "\n")
    
    return metrics['eval_accuracy']

# =============================================================================
# 3. ëª¨ë¸ í•™ìŠµ
# =============================================================================
def train_model(epochs=10):
    print("\n" + "="*50)
    print(f"[Step 3] ëª¨ë¸ í•™ìŠµ ì‹œì‘ ({epochs} epochs)...")
    print("="*50)
    
    if train_dataset is None:
        print("[Error] ë¨¼ì € ë°ì´í„°ë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš”")
        return None
    
    model = ASTForAudioClassification.from_pretrained(
        MODEL_NAME,
        num_labels=len(labels),
        label2id=label2id,
        id2label=id2label,
        ignore_mismatched_sizes=True
    )
    
    accuracy_metric = evaluate.load("accuracy")
    
    def compute_metrics(eval_pred):
        predictions, labels_arr = eval_pred
        predictions = np.argmax(predictions, axis=1)
        return accuracy_metric.compute(predictions=predictions, references=labels_arr)
    
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=8,
        num_train_epochs=epochs,
        learning_rate=3e-5,
        logging_dir='./logs',
        evaluation_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",
        push_to_hub=False,
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        compute_metrics=compute_metrics,
    )
    
    print("í•™ìŠµ ì‹œì‘...")
    trainer.train()
    
    # ëª¨ë¸ ì €ì¥
    os.makedirs(SAVE_PATH, exist_ok=True)
    model.save_pretrained(SAVE_PATH)
    feature_extractor.save_pretrained(SAVE_PATH)
    print(f"[âœ“] ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {SAVE_PATH}")
    
    return trainer

# =============================================================================
# 4. ìµœì¢… ëª¨ë¸ í…ŒìŠ¤íŠ¸
# =============================================================================
def evaluate_final():
    print("\n" + "="*50)
    print("[Step 4] ìµœì¢… ëª¨ë¸ ì •ë°€ë„ ì¸¡ì •...")
    print("="*50)
    
    if not os.path.exists(SAVE_PATH):
        print(f"[Error] í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤: {SAVE_PATH}")
        print("ë¨¼ì € --mode train ìœ¼ë¡œ í•™ìŠµì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    if test_dataset is None:
        print("[Error] ë¨¼ì € ë°ì´í„°ë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš”")
        return
    
    # í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
    model = ASTForAudioClassification.from_pretrained(SAVE_PATH)
    
    accuracy_metric = evaluate.load("accuracy")
    
    def compute_metrics(eval_pred):
        predictions, labels_arr = eval_pred
        predictions = np.argmax(predictions, axis=1)
        return accuracy_metric.compute(predictions=predictions, references=labels_arr)
    
    training_args = TrainingArguments(
        output_dir="./ai/runs/final_eval",
        per_device_eval_batch_size=8,
        push_to_hub=False,
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        eval_dataset=test_dataset,
        compute_metrics=compute_metrics,
    )
    
    metrics = trainer.evaluate()
    
    print("\n" + "="*40)
    print(f"ğŸ¯ ìµœì¢… ëª¨ë¸ ì •í™•ë„(Final): {metrics['eval_accuracy']:.4f}")
    print("="*40 + "\n")
    
    return metrics['eval_accuracy']

# =============================================================================
# Main
# =============================================================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="AST Audio Model Training Script")
    parser.add_argument("--mode", type=str, default="all",
                        choices=["baseline", "train", "test", "all"],
                        help="ì‹¤í–‰ ëª¨ë“œ: baseline(ì´ˆê¸°), train(í•™ìŠµ), test(í…ŒìŠ¤íŠ¸), all(ì „ì²´)")
    parser.add_argument("--epochs", type=int, default=10,
                        help="í•™ìŠµ ì—í­ ìˆ˜ (ê¸°ë³¸ê°’: 10)")
    
    args = parser.parse_args()
    
    print(f"\nğŸš€ Audio Training Script ì‹œì‘ (mode={args.mode}, epochs={args.epochs})")
    
    # ë°ì´í„° ì¤€ë¹„ (ëª¨ë“  ëª¨ë“œì—ì„œ í•„ìš”)
    if not prepare_data():
        exit(1)
    
    if args.mode == "baseline":
        evaluate_baseline()
    
    elif args.mode == "train":
        train_model(epochs=args.epochs)
    
    elif args.mode == "test":
        evaluate_final()
    
    elif args.mode == "all":
        baseline_acc = evaluate_baseline()
        train_model(epochs=args.epochs)
        final_acc = evaluate_final()
        
        print("\n" + "="*50)
        print("ğŸ“Š ì •í™•ë„ ë¹„êµ")
        print("="*50)
        print(f"   ì´ˆê¸° ëª¨ë¸(Baseline): {baseline_acc:.4f}")
        print(f"   ìµœì¢… ëª¨ë¸(Final):    {final_acc:.4f}")
        print(f"   í–¥ìƒë„:              +{(final_acc - baseline_acc)*100:.2f}%")
        print("="*50 + "\n")
    
    print("âœ… ì™„ë£Œ!")