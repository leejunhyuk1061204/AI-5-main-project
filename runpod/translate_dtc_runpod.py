import json
import os
import sqlite3
import asyncio
import aiohttp
import sys
from tqdm import tqdm
from automotive_terms import AUTOMOTIVE_TERMS

# í•œê¸€ ì£¼ì„: DTC ë°ì´í„°ë¥¼ í†µí•© ë¡œë“œí•˜ì—¬ Ollama Qwen2.5ë¥¼ í†µí•´ ê³ í’ˆì§ˆ í•œê¸€ ë²ˆì—­ ë° TTS ë¬¸êµ¬ë¥¼ ìƒì„±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

# --- ì„¤ì • (Configuration) ---
OLLAMA_URL = "http://localhost:11434/api/generate"
OLLAMA_CHECK_URL = "http://localhost:11434"
MODEL_NAME = "qwen2.5:14b"  # 14B + ìš©ì–´ì‚¬ì „ = ì†ë„/í’ˆì§ˆ ìµœì  ë°¸ëŸ°ìŠ¤
BATCH_SIZE = 30  # 14BëŠ” ê°€ë²¼ì›Œì„œ 30ê°œ ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ìœ„ì¹˜ ê¸°ì¤€ ê²½ë¡œ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")

DATA_SOURCES = {
    # runpod/data í´ë” êµ¬ì¡°ì— ë§ì¶¤ (data/íŒŒì¼ëª…)
    "bulk": os.path.join(DATA_DIR, "github_dtc_bulk.json"),
    "backup": os.path.join(DATA_DIR, "github_dtc_bulk_ko_backup.json"),
    "summary": os.path.join(DATA_DIR, "batch_dtc_summary.json"),
    "sample": os.path.join(DATA_DIR, "dry_run_sample.json"),
    "db": os.path.join(DATA_DIR, "github_dtc_codes.db")
}

CACHE_FILE = os.path.join(DATA_DIR, "translated_cache.json")
FINAL_FILE = os.path.join(DATA_DIR, "translated_dtc_final.json")

# --- ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (í€„ë¦¬í‹° ì¤‘ì‹¬) ---
def get_system_prompt():
    terms_str = "\n".join([f"- {en}: {ko}" for en, ko in AUTOMOTIVE_TERMS.items()])
    return f"""ë„ˆëŠ” ìˆ™ë ¨ëœ ìë™ì°¨ ì •ë¹„ ì „ë¬¸ê°€ì´ì TTS(Text-to-Speech) ì „ë¬¸ ì„±ìš°ì•¼.
ì œê³µëœ [ìë™ì°¨ ìš©ì–´ ì‚¬ì „]ì„ ì°¸ê³ í•´ì„œ ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ì§€ì¼œì„œ ë²ˆì—­í•´ì¤˜:

1. ì „ë¬¸ ìš©ì–´ëŠ” ì‚¬ì „ì— ì •ì˜ëœ í‘œì¤€ ë²ˆì—­ì–´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì‚¬ìš©í•˜ë˜, ë¬¸ë§¥ì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•´ì¤˜.
2. **TTS ìµœì í™”**: ìš´ì „ìê°€ ìš´ì „ ì¤‘ì— ë“¤ì–´ë„ í•œ ë²ˆì— ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë¶€ë“œëŸ¬ìš´ êµ¬ì–´ì²´(~ì…ë‹ˆë‹¤, ~ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ ë“±) ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•´ì¤˜.
3. **ì–¸ì–´ ì œí•œ**: **ì ˆëŒ€ ì¤‘êµ­ì–´(í•œì)ë¥¼ í¬í•¨í•˜ì§€ ë§ˆ.** ê²°ê³¼ëŠ” ë¬´ì¡°ê±´ **í•œê¸€**ì´ì–´ì•¼ í•´. (ì˜ë¬¸ ì•½ì–´ëŠ” í—ˆìš©)
4. **êµ¬ì¡°í™”ëœ ì¶œë ¥**: ë°˜ë“œì‹œ ì•„ë˜ì˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì¤˜. ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆ.

[ìë™ì°¨ ìš©ì–´ ì‚¬ì „]
{terms_str}

[ì¶œë ¥ JSON í˜•ì‹ êµë³¸]
{{
    "translated": "í•œê¸€ ë²ˆì—­ë¬¸",
    "tts_phrase": "ìš´ì „ìë¥¼ ìœ„í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì•ˆë‚´ ë¬¸êµ¬",
    "summary": "í•µì‹¬ ìš”ì•½ (5ì ì´ë‚´)"
}}
"""

async def check_ollama_server():
    """Ollama ì„œë²„ ì—°ê²° í™•ì¸"""
    print(f"Connecting to Ollama at {OLLAMA_CHECK_URL}...")
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(OLLAMA_CHECK_URL, timeout=5) as response:
                if response.status == 200:
                    print("âœ… Ollama server is ready.")
                    return True
                else:
                    print(f"âŒ Ollama server responded with status {response.status}")
                    return False
        except Exception as e:
            print(f"âŒ Could not connect to Ollama: {e}")
            return False

async def translate_item(session, code, original, system_prompt):
    """Ollama APIë¥¼ í†µí•´ ë‹¨ì¼ í•­ëª© ë²ˆì—­ ìš”ì²­"""
    prompt = f"DTC ì½”ë“œ: {code}\nì˜ë¬¸ ì›ë¬¸: {original}\n\nìœ„ ë‚´ìš©ì„ ê·œì¹™ì— ë§ì¶° ë²ˆì—­í•´ì¤˜."
    
    payload = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "system": system_prompt,
        "stream": False,
        "format": "json",
        "options": {
            "num_ctx": 8192  # ìš©ì–´ ì‚¬ì „ì´ ê¸°ë‹ˆê¹Œ ë¬¸ë§¥ ê¸¸ì´ë¥¼ 2ë°°ë¡œ ë˜ ëŠ˜ë¦¼ (ì˜ë¦¼ ë°©ì§€)
        }
    }
    
    try:
        # 32B ëª¨ë¸ ì²˜ë¦¬ ì‹œê°„ì„ ê³ ë ¤í•´ íƒ€ì„ì•„ì›ƒì„ 5ë¶„(300ì´ˆ)ìœ¼ë¡œ ëŒ€í­ ëŠ˜ë¦¼
        async with session.post(OLLAMA_URL, json=payload, timeout=300) as response:
            if response.status == 200:
                result = await response.json()
                return json.loads(result.get("response", "{}"))
            else:
                return None
    except Exception as e:
        print(f"\nError translating {code}: {e}")
        return None

def load_all_data():
    """ëª¨ë“  ì†ŒìŠ¤ íŒŒì¼ì—ì„œ DTC ë°ì´í„°ë¥¼ í†µí•© ë¡œë“œ (ì¤‘ë³µ ì œê±°)"""
    all_dtcs = {} # code_original_hash -> {code, original, category}
    
    # helper to process list or dict
    def process_items(source_key, category_type):
        path = DATA_SOURCES.get(source_key)
        if not path or not os.path.exists(path):
            return
            
        print(f"Loading {source_key} from {path}...")
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            items = data if isinstance(data, list) else [data]
            for item in items:
                code = item.get('code', 'UNKNOWN')
                orig = item.get('original_context', '')
                # Skip if already translated properly (optional check, but we load everything here)
                if orig:
                    key = f"{code}_{orig}"
                    all_dtcs[key] = {"code": code, "original": orig, "category": category_type}
        except Exception as e:
            print(f"Error loading {source_key}: {e}")

    # 1. Bulk & Backup (Plain definitions)
    process_items("bulk", "DEFINITION")
    process_items("backup", "DEFINITION")

    # 2. Summary & Sample (Long context)
    process_items("summary", "SUMMARY")
    process_items("sample", "SUMMARY")

    # 3. SQLite DB
    if os.path.exists(DATA_SOURCES["db"]):
        try:
            conn = sqlite3.connect(DATA_SOURCES["db"])
            cursor = conn.cursor()
            cursor.execute("SELECT code, description FROM dtc_definitions")
            for code, desc in cursor.fetchall():
                if desc:
                    key = f"{code}_{desc}"
                    all_dtcs[key] = {"code": code, "original": desc, "category": "DB_DEFINITION"}
            conn.close()
        except Exception as e:
             print(f"âš ï¸ Warning: Error reading DB at {DATA_SOURCES['db']}: {e}")
    else:
        print(f"âš ï¸ Warning: DB not found at {DATA_SOURCES['db']}")

    return list(all_dtcs.values())

async def main():
    # ëª¨ë¸ í™•ì¸ì€ ì´ë¯¸ run.shì—ì„œ ollama pullë¡œ ë³´ì¥í•˜ë ¤ í•˜ì§€ë§Œ, ì—°ê²° í™•ì¸ ì°¨ì›
    if not await check_ollama_server():
        print("âŒ Please ensure Ollama is running (try 'ollama serve' or check logs).")
        return

    # ë°ì´í„° ë¡œë“œ
    items = load_all_data()
    print(f"Total unique DTC items to translate: {len(items)}")

    if not items:
        print("âŒ No data found to translate. Check 'data' folder.")
        return

    # ê²°ê³¼ë¬¼ ì €ì¥ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(os.path.dirname(FINAL_FILE), exist_ok=True)

    # ìºì‹œ ë¡œë“œ
    cache = {}
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, 'r', encoding='utf-8') as f:
            cache = json.load(f)
        
        # [ì¤‘êµ­ì–´ ì˜¤ì—¼ ë°ì´í„° ìë™ ì„¸ì²™]
        # ì´ë¯¸ í•œìê°€ í¬í•¨ëœ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ìºì‹œì—ì„œ ì‚­ì œí•˜ì—¬ ì¬ë²ˆì—­ ìœ ë„
        import re
        dirty_keys = []
        # í•œì ìœ ë‹ˆì½”ë“œ ë²”ìœ„: \u4e00-\u9fff
        chinese_pattern = re.compile(r'[\u4e00-\u9fff]')
        
        print("Checking cache for Chinese character contamination...")
        for k, v in cache.items():
            translated_text = v.get('translated', '')
            tts_text = v.get('tts_phrase', '')
            summary_text = v.get('summary', '')
            
            # í•œìê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì˜¤ì—¼ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
            if (chinese_pattern.search(translated_text) or 
                chinese_pattern.search(tts_text) or 
                chinese_pattern.search(summary_text)):
                dirty_keys.append(k)
        
        if dirty_keys:
            print(f"ğŸ§¹ Found {len(dirty_keys)} entries with Chinese characters. Removing them to re-translate...")
            for k in dirty_keys:
                del cache[k]
            # ì„¸ì²™ëœ ìºì‹œ ì €ì¥
            with open(CACHE_FILE, 'w', encoding='utf-8') as f:
                json.dump(cache, f, ensure_ascii=False, indent=2)
        else:
            print("âœ¨ Cache is clean (no Chinese characters found).")
            
    # ë¯¸ë²ˆì—­ í•­ëª© ì„ ë³„
    to_translate = []
    for item in items:
        key = f"{item['code']}_{item['original']}"
        if key not in cache:
            to_translate.append(item)
    
    print(f"Remaining items to translate: {len(to_translate)}")
    if not to_translate:
        print("All items translated!")
        # ê·¸ë˜ë„ FINAL íŒŒì¼ì€ ìƒì„±í•´ì•¼ í•¨ (ìºì‹œ ë‚´ìš©ìœ¼ë¡œ)
        print(f"Saving final results to {FINAL_FILE}...")
        with open(FINAL_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
        return

    system_prompt = get_system_prompt()
    
    async with aiohttp.ClientSession() as session:
        # BATCH_SIZE ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ë˜, tqdmì€ ì•„ì´í…œ ë‹¨ìœ„ë¡œ í‘œì‹œ
        pbar = tqdm(total=len(to_translate), desc="Translating Items")
        for i in range(0, len(to_translate), BATCH_SIZE):
            batch = to_translate[i:i+BATCH_SIZE]
            tasks = [translate_item(session, item['code'], item['original'], system_prompt) for item in batch]
            results = await asyncio.gather(*tasks)
            
            # ê²°ê³¼ ì €ì¥ (ìºì‹œ ì—…ë°ì´íŠ¸)
            changed = False
            for item, res in zip(batch, results):
                if res:
                    key = f"{item['code']}_{item['original']}"
                    cache[key] = {
                        "code": item['code'],
                        "original": item['original'],
                        "category": item['category'],
                        "translated": res.get("translated", ""),
                        "tts_phrase": res.get("tts_phrase", ""),
                        "summary": res.get("summary", "")
                    }
                    changed = True
            
            # ì£¼ê¸°ì ìœ¼ë¡œ ìºì‹œ íŒŒì¼ ì €ì¥
            if changed:
                with open(CACHE_FILE, 'w', encoding='utf-8') as f:
                    json.dump(cache, f, ensure_ascii=False, indent=2)
            
            pbar.update(len(batch))
        pbar.close()

    # ìµœì¢… ê²°ê³¼ íŒŒì¼ ìƒì„± (í†µí•©ë³¸)
    print(f"Saving combined results to {FINAL_FILE}...")
    with open(FINAL_FILE, 'w', encoding='utf-8') as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)
    
    # ---------------------------------------------------------
    # 4. ì†ŒìŠ¤ë³„ ê°œë³„ íŒŒì¼ ë¶„ë¦¬ ì €ì¥ (ì‚¬ìš©ì ìš”ì²­ ì‚¬í•­)
    # ---------------------------------------------------------
    print("\n--- Exporting individual files ---")
    for source_key, source_path in DATA_SOURCES.items():
        if not os.path.exists(source_path): continue
        
        # ì´ë¦„ ìƒì„±: github_dtc_bulk.json -> github_dtc_bulk_translated.json
        base_name = os.path.splitext(source_path)[0]
        output_filename = f"{base_name}_translated.json"
        
        print(f"Exporting {source_key} -> {output_filename}...")
        
        try:
            with open(source_path, 'r', encoding='utf-8') as f:
                original_data = json.load(f)
            
            # ë¦¬ìŠ¤íŠ¸ë‚˜ ë”•ì…”ë„ˆë¦¬ ì²˜ë¦¬ (DB íŒŒì¼ ë“± êµ¬ì¡°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
            # ì—¬ê¸°ì„œëŠ” JSON íŒŒì¼ë“¤ë§Œ ì²˜ë¦¬í•œë‹¤ê³  ê°€ì • (DBëŠ” ìœ„ì—ì„œ ì½ê¸°ë§Œ í–ˆìœ¼ë¯€ë¡œ íŒ¨ìŠ¤í•˜ê±°ë‚˜ ë³„ë„ ì²˜ë¦¬ í•„ìš”í•˜ì§€ë§Œ, 
            # ì‚¬ìš©ì ìš”ì²­ íŒŒì¼ 4ê°œëŠ” ëª¨ë‘ JSONì´ë¯€ë¡œ í†µìš©ë¨)
            
            export_list = []
            
            # DBì˜ ê²½ìš° original_dataê°€ listê°€ ì•„ë‹ ìˆ˜ ìˆìŒ (ìœ„ load_all_dataëŠ” DB ì§ì ‘ ì ‘ì†í•¨)
            # ë”°ë¼ì„œ JSON íŒŒì¼ì¸ ê²½ìš°ë§Œ ì²˜ë¦¬
            if source_key == "db": 
                continue 

            items = original_data if isinstance(original_data, list) else [original_data]
            
            for item in items:
                code = item.get('code', 'UNKNOWN')
                orig = item.get('original_context', '')
                
                # ì›ë³¸ ë³µì‚¬
                new_item = item.copy()
                
                # ìºì‹œì—ì„œ ë²ˆì—­ ì°¾ê¸°
                key = f"{code}_{orig}"
                if key in cache:
                    cached_item = cache[key]
                    if cached_item.get('translated'):
                        new_item['korean_translation'] = cached_item['translated']
                        new_item['tts_phrase'] = cached_item['tts_phrase']
                        new_item['summary_ko'] = cached_item['summary']
                
                export_list.append(new_item)
                    
            with open(output_filename, 'w', encoding='utf-8') as f:
                json.dump(export_list, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"Error exporting {source_key}: {e}")

    print("\nTranslation & Export completed successfully!")

if __name__ == "__main__":
    asyncio.run(main())
